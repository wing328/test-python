# coding: utf-8

"""
    Cisco Intersight

    Cisco Intersight is a management platform delivered as a service with embedded analytics for your Cisco and 3rd party IT infrastructure. This platform offers an intelligent level of management that enables IT organizations to analyze, simplify, and automate their environments in more advanced ways than the prior generations of tools. Cisco Intersight provides an integrated and intuitive management experience for resources in the traditional data center as well as at the edge. With flexible deployment options to address complex security needs, getting started with Intersight is quick and easy. Cisco Intersight has deep integration with Cisco UCS and HyperFlex systems allowing for remote deployment, configuration, and ongoing maintenance. The model-based deployment works for a single system in a remote location or hundreds of systems in a data center and enables rapid, standardized configuration and deployment. It also streamlines maintaining those systems whether you are working with small or very large configurations. The Intersight OpenAPI document defines the complete set of properties that are returned in the HTTP response. From that perspective, a client can expect that no additional properties are returned, unless these properties are explicitly defined in the OpenAPI document. However, when a client uses an older version of the Intersight OpenAPI document, the server may send additional properties because the software is more recent than the client. In that case, the client may receive properties that it does not know about. Some generated SDKs perform a strict validation of the HTTP response body against the OpenAPI document. This document was created on 2020-06-30T07:31:54Z.  # noqa: E501

    The version of the OpenAPI document: 1.0.9-1950
    Contact: intersight@cisco.com
    Generated by: https://openapi-generator.tech
"""


import re  # noqa: F401
import sys  # noqa: F401

import nulltype  # noqa: F401

from openapi_client.model_utils import (  # noqa: F401
    ApiTypeError,
    ModelComposed,
    ModelNormal,
    ModelSimple,
    cached_property,
    change_keys_js_to_python,
    convert_js_args_to_python_args,
    date,
    datetime,
    file_type,
    none_type,
    validate_get_composed_info,
)

def lazy_import():
    from openapi_client.model.telemetry_druid_aggregator import TelemetryDruidAggregator
    from openapi_client.model.telemetry_druid_data_source import TelemetryDruidDataSource
    from openapi_client.model.telemetry_druid_data_source_metadata_request import TelemetryDruidDataSourceMetadataRequest
    from openapi_client.model.telemetry_druid_default_limit_spec import TelemetryDruidDefaultLimitSpec
    from openapi_client.model.telemetry_druid_dimension_spec import TelemetryDruidDimensionSpec
    from openapi_client.model.telemetry_druid_filter import TelemetryDruidFilter
    from openapi_client.model.telemetry_druid_granularity import TelemetryDruidGranularity
    from openapi_client.model.telemetry_druid_group_by_request import TelemetryDruidGroupByRequest
    from openapi_client.model.telemetry_druid_having_filter import TelemetryDruidHavingFilter
    from openapi_client.model.telemetry_druid_post_aggregator import TelemetryDruidPostAggregator
    from openapi_client.model.telemetry_druid_query_context import TelemetryDruidQueryContext
    from openapi_client.model.telemetry_druid_scan_request import TelemetryDruidScanRequest
    from openapi_client.model.telemetry_druid_segment_metadata_request import TelemetryDruidSegmentMetadataRequest
    from openapi_client.model.telemetry_druid_time_boundary_request import TelemetryDruidTimeBoundaryRequest
    from openapi_client.model.telemetry_druid_time_series_request import TelemetryDruidTimeSeriesRequest
    from openapi_client.model.telemetry_druid_top_n_metric_spec import TelemetryDruidTopNMetricSpec
    from openapi_client.model.telemetry_druid_top_n_request import TelemetryDruidTopNRequest
    globals()['TelemetryDruidAggregator'] = TelemetryDruidAggregator
    globals()['TelemetryDruidDataSource'] = TelemetryDruidDataSource
    globals()['TelemetryDruidDataSourceMetadataRequest'] = TelemetryDruidDataSourceMetadataRequest
    globals()['TelemetryDruidDefaultLimitSpec'] = TelemetryDruidDefaultLimitSpec
    globals()['TelemetryDruidDimensionSpec'] = TelemetryDruidDimensionSpec
    globals()['TelemetryDruidFilter'] = TelemetryDruidFilter
    globals()['TelemetryDruidGranularity'] = TelemetryDruidGranularity
    globals()['TelemetryDruidGroupByRequest'] = TelemetryDruidGroupByRequest
    globals()['TelemetryDruidHavingFilter'] = TelemetryDruidHavingFilter
    globals()['TelemetryDruidPostAggregator'] = TelemetryDruidPostAggregator
    globals()['TelemetryDruidQueryContext'] = TelemetryDruidQueryContext
    globals()['TelemetryDruidScanRequest'] = TelemetryDruidScanRequest
    globals()['TelemetryDruidSegmentMetadataRequest'] = TelemetryDruidSegmentMetadataRequest
    globals()['TelemetryDruidTimeBoundaryRequest'] = TelemetryDruidTimeBoundaryRequest
    globals()['TelemetryDruidTimeSeriesRequest'] = TelemetryDruidTimeSeriesRequest
    globals()['TelemetryDruidTopNMetricSpec'] = TelemetryDruidTopNMetricSpec
    globals()['TelemetryDruidTopNRequest'] = TelemetryDruidTopNRequest


class TelemetryDruidAggregateRequest(ModelComposed):
    """NOTE: This class is auto generated by OpenAPI Generator.
    Ref: https://openapi-generator.tech

    Do not edit the class manually.

    Attributes:
      allowed_values (dict): The key is the tuple path to the attribute
          and the for var_name this is (var_name,). The value is a dict
          with a capitalized key describing the allowed value and an allowed
          value. These dicts store the allowed enum values.
      attribute_map (dict): The key is attribute name
          and the value is json key in definition.
      discriminator_value_class_map (dict): A dict to go from the discriminator
          variable value to the discriminator class name.
      validations (dict): The key is the tuple path to the attribute
          and the for var_name this is (var_name,). The value is a dict
          that stores validations for max_length, min_length, max_items,
          min_items, exclusive_maximum, inclusive_maximum, exclusive_minimum,
          inclusive_minimum, and regex.
      additional_properties_type (tuple): A tuple of classes accepted
          as additional properties values.
    """

    allowed_values = {
        ('query_type',): {
            'TIMESERIES': "timeseries",
            'TOPN': "topN",
            'GROUPBY': "groupBy",
            'SCAN': "scan",
            'TIMEBOUNDARY': "timeBoundary",
            'SEGMENTMETADATA': "segmentMetadata",
            'DATASOURCEMETADATA': "dataSourceMetadata",
            'SEARCH': "search",
        },
        ('result_format',): {
            'LIST': "list",
            'COMPACTEDLIST': "compactedList",
        },
        ('order',): {
            'NONE': "none",
            'ASCENDING': "ascending",
            'DESCENDING': "descending",
        },
        ('bound',): {
            'MAXTIME': "maxTime",
            'MINTIME': "minTime",
        },
    }

    validations = {
    }

    @cached_property
    def additional_properties_type():
        """
        This must be a method because a model may have properties that are
        of type self, this must run after the class is loaded
        """
        lazy_import()
        return (bool, date, datetime, dict, float, int, list, str, none_type,)  # noqa: E501

    _nullable = False

    @cached_property
    def openapi_types():
        """
        This must be a method because a model may have properties that are
        of type self, this must run after the class is loaded

        Returns
            openapi_types (dict): The key is attribute name
                and the value is attribute type.
        """
        lazy_import()
        return {
            'query_type': (str,),  # noqa: E501
            'data_source': (TelemetryDruidDataSource,),  # noqa: E501
            'intervals': ([str],),  # noqa: E501
            'granularity': (TelemetryDruidGranularity,),  # noqa: E501
            'dimension': (TelemetryDruidDimensionSpec,),  # noqa: E501
            'threshold': (int,),  # noqa: E501
            'metric': (TelemetryDruidTopNMetricSpec,),  # noqa: E501
            'dimensions': ([TelemetryDruidDimensionSpec],),  # noqa: E501
            'descending': (bool,),  # noqa: E501
            'filter': (TelemetryDruidFilter,),  # noqa: E501
            'aggregations': ([TelemetryDruidAggregator],),  # noqa: E501
            'post_aggregations': ([TelemetryDruidPostAggregator],),  # noqa: E501
            'limit': (int,),  # noqa: E501
            'context': (TelemetryDruidQueryContext,),  # noqa: E501
            'limit_spec': (TelemetryDruidDefaultLimitSpec,),  # noqa: E501
            'having': (TelemetryDruidHavingFilter,),  # noqa: E501
            'subtotals_spec': ({str: (bool, date, datetime, dict, float, int, list, str, none_type)},),  # noqa: E501
            'result_format': (str,),  # noqa: E501
            'columns': ([str],),  # noqa: E501
            'batch_size': (int,),  # noqa: E501
            'order': (str,),  # noqa: E501
            'legacy': (bool,),  # noqa: E501
            'bound': (str,),  # noqa: E501
            'to_include': ({str: (bool, date, datetime, dict, float, int, list, str, none_type)},),  # noqa: E501
            'merge': (bool,),  # noqa: E501
            'analysis_types': ([str],),  # noqa: E501
            'lenient_aggregator_merge': (bool,),  # noqa: E501
        }

    @cached_property
    def discriminator():
        lazy_import()
        val = {
            'dataSourceMetadata': TelemetryDruidDataSourceMetadataRequest,
            'groupBy': TelemetryDruidGroupByRequest,
            'scan': TelemetryDruidScanRequest,
            'segmentMetadata': TelemetryDruidSegmentMetadataRequest,
            'telemetry.DruidDataSourceMetadataRequest': TelemetryDruidDataSourceMetadataRequest,
            'telemetry.DruidGroupByRequest': TelemetryDruidGroupByRequest,
            'telemetry.DruidScanRequest': TelemetryDruidScanRequest,
            'telemetry.DruidSegmentMetadataRequest': TelemetryDruidSegmentMetadataRequest,
            'telemetry.DruidTimeBoundaryRequest': TelemetryDruidTimeBoundaryRequest,
            'telemetry.DruidTimeSeriesRequest': TelemetryDruidTimeSeriesRequest,
            'telemetry.DruidTopNRequest': TelemetryDruidTopNRequest,
            'timeBoundary': TelemetryDruidTimeBoundaryRequest,
            'timeseries': TelemetryDruidTimeSeriesRequest,
            'topN': TelemetryDruidTopNRequest,
        }
        if not val:
            return None
        return {'query_type': val}

    attribute_map = {
        'query_type': 'queryType',  # noqa: E501
        'data_source': 'dataSource',  # noqa: E501
        'intervals': 'intervals',  # noqa: E501
        'granularity': 'granularity',  # noqa: E501
        'dimension': 'dimension',  # noqa: E501
        'threshold': 'threshold',  # noqa: E501
        'metric': 'metric',  # noqa: E501
        'dimensions': 'dimensions',  # noqa: E501
        'descending': 'descending',  # noqa: E501
        'filter': 'filter',  # noqa: E501
        'aggregations': 'aggregations',  # noqa: E501
        'post_aggregations': 'postAggregations',  # noqa: E501
        'limit': 'limit',  # noqa: E501
        'context': 'context',  # noqa: E501
        'limit_spec': 'limitSpec',  # noqa: E501
        'having': 'having',  # noqa: E501
        'subtotals_spec': 'subtotalsSpec',  # noqa: E501
        'result_format': 'resultFormat',  # noqa: E501
        'columns': 'columns',  # noqa: E501
        'batch_size': 'batchSize',  # noqa: E501
        'order': 'order',  # noqa: E501
        'legacy': 'legacy',  # noqa: E501
        'bound': 'bound',  # noqa: E501
        'to_include': 'toInclude',  # noqa: E501
        'merge': 'merge',  # noqa: E501
        'analysis_types': 'analysisTypes',  # noqa: E501
        'lenient_aggregator_merge': 'lenientAggregatorMerge',  # noqa: E501
    }

    required_properties = set([
        '_data_store',
        '_check_type',
        '_spec_property_naming',
        '_path_to_item',
        '_configuration',
        '_visited_composed_classes',
        '_composed_instances',
        '_var_name_to_model_instances',
        '_additional_properties_model_instances',
    ])

    @convert_js_args_to_python_args
    def __init__(self, query_type, *args, **kwargs):  # noqa: E501
        """TelemetryDruidAggregateRequest - a model defined in OpenAPI

        Args:
            query_type (str): null

        Keyword Args:
            data_source (TelemetryDruidDataSource): defaults to nulltype.Null  # noqa: E501
            intervals ([str]): A JSON Object representing ISO-8601 Intervals. This defines the time ranges to run the query over. If an interval is not specified, the query will use a default interval that spans a configurable period before the end time of the most recent segment.. defaults to nulltype.Null  # noqa: E501
            granularity (TelemetryDruidGranularity): defaults to nulltype.Null  # noqa: E501
            dimension (TelemetryDruidDimensionSpec): defaults to nulltype.Null  # noqa: E501
            threshold (int): An integer defining the N in the topN (i.e. how many results you want in the top list).. defaults to nulltype.Null  # noqa: E501
            metric (TelemetryDruidTopNMetricSpec): defaults to nulltype.Null  # noqa: E501
            dimensions ([TelemetryDruidDimensionSpec]): A JSON list of dimensions to do the groupBy over; or see DimensionSpec for ways to extract dimensions... defaults to nulltype.Null  # noqa: E501
            _check_type (bool): if True, values for parameters in openapi_types
                                will be type checked and a TypeError will be
                                raised if the wrong type is input.
                                Defaults to True
            _path_to_item (tuple/list): This is a list of keys or values to
                                drill down to the model in received_data
                                when deserializing a response
            _spec_property_naming (bool): True if the variable names in the input data
                                are serialized names, as specified in the OpenAPI document.
                                False if the variable names in the input data
                                are pythonic names, e.g. snake case (default)
            _configuration (Configuration): the instance to use when
                                deserializing a file_type parameter.
                                If passed, type conversion is attempted
                                If omitted no type conversion is done.
            _visited_composed_classes (tuple): This stores a tuple of
                                classes that we have traveled through so that
                                if we see that class again we will not use its
                                discriminator again.
                                When traveling through a discriminator, the
                                composed schema that is
                                is traveled through is added to this set.
                                For example if Animal has a discriminator
                                petType and we pass in "Dog", and the class Dog
                                allOf includes Animal, we move through Animal
                                once using the discriminator, and pick Dog.
                                Then in Dog, we will make an instance of the
                                Animal class but this time we won't travel
                                through its discriminator because we passed in
                                _visited_composed_classes = (Animal,)
            descending (bool): Whether to make descending ordered result. Default is false(ascending).. [optional]  # noqa: E501
            filter (TelemetryDruidFilter): [optional]  # noqa: E501
            aggregations ([TelemetryDruidAggregator]): Aggregation functions are used to summarize data in buckets. Summarization functions include counting rows, calculating the min/max/sum of metrics and retrieving the first/last value of metrics for each bucket. Additional summarization functions are available with extensions. If no aggregator is provided, the results will be empty for each bucket.. [optional]  # noqa: E501
            post_aggregations ([TelemetryDruidPostAggregator]): Post-aggregations are specifications of processing that should happen on aggregated values as they come out of Apache Druid. If you include a post aggregation as part of a query, make sure to include all aggregators the post-aggregator requires.. [optional]  # noqa: E501
            limit (int): How many rows to return. If not specified, all rows will be returned.. [optional]  # noqa: E501
            context (TelemetryDruidQueryContext): [optional]  # noqa: E501
            limit_spec (TelemetryDruidDefaultLimitSpec): [optional]  # noqa: E501
            having (TelemetryDruidHavingFilter): [optional]  # noqa: E501
            subtotals_spec ({str: (bool, date, datetime, dict, float, int, list, str, none_type)}): A JSON array of arrays to return additional result sets for groupings of subsets of top level dimensions. The subtotals feature allows computation of multiple sub-groupings in a single query. To use this feature, add a \&quot;subtotalsSpec\&quot; to your query, which should be a list of subgroup dimension sets. It should contain the \&quot;outputName\&quot; from dimensions in your \&quot;dimensions\&quot; attribute, in the same order as they appear in the \&quot;dimensions\&quot; attribute.. [optional]  # noqa: E501
            result_format (str): How the results are represented, list, compactedList or valueVector. Currently only list and compactedList are supported.. [optional] if omitted the server will use the default value of "list"  # noqa: E501
            columns ([str]): A String array of dimensions and metrics to scan. If left empty, all dimensions and metrics are returned.. [optional]  # noqa: E501
            batch_size (int): The maximum number of rows buffered before being returned to the client.. [optional] if omitted the server will use the default value of 20480  # noqa: E501
            order (str): The ordering of returned rows based on timestamp. \&quot;ascending\&quot;, \&quot;descending\&quot;, and \&quot;none\&quot; (default) are supported. Currently, \&quot;ascending\&quot; and \&quot;descending\&quot; are only supported for queries where the __time column is included in the columns field and the requirements outlined in the time ordering section are met.. [optional] if omitted the server will use the default value of "none"  # noqa: E501
            legacy (bool): Return results consistent with the legacy \&quot;scan-query\&quot; contrib extension. Defaults to the value set by druid.query.scan.legacy, which in turn defaults to false.. [optional] if omitted the server will use the default value of False  # noqa: E501
            bound (str): Optional, set to maxTime or minTime to return only the latest or earliest timestamp. Default to returning both if not set.. [optional]  # noqa: E501
            to_include ({str: (bool, date, datetime, dict, float, int, list, str, none_type)}): A JSON Object representing what columns should be included in the result. Defaults to \&quot;all\&quot;.. [optional]  # noqa: E501
            merge (bool): Merge all individual segment metadata results into a single result.. [optional]  # noqa: E501
            analysis_types ([str]): A list of Strings specifying what column properties (e.g. cardinality, size) should be calculated and returned in the result. Defaults to [\&quot;cardinality\&quot;, \&quot;interval\&quot;, \&quot;minmax\&quot;], but can be overridden with using the segment metadata query config. * cardinality - in the result will return the estimated floor of cardinality for each column. Only relevant for dimension columns. * minmax - Estimated min/max values for each column. Only relevant for dimension columns. * size - in the result will contain the estimated total segment byte size as if the data were stored in text format. * intervals - in the result will contain the list of intervals associated with the queried segments. * timestampSpec - in the result will contain timestampSpec of data stored in segments. This can be null if timestampSpec of segments was unknown or unmergeable (if merging is enabled). * queryGranularity - in the result will contain query granularity of data stored in segments. This can be null if query granularity of segments was unknown or unmergeable (if merging is enabled). * aggregators - in the result will contain the list of aggregators usable for querying metric columns. This may be null if the aggregators are unknown or unmergeable (if merging is enabled). Merging can be strict or lenient. The form of the result is a map of column name to aggregator. * rollup - in the result is true/false/null. When merging is enabled, if some are rollup, others are not, result is null.. [optional]  # noqa: E501
            lenient_aggregator_merge (bool): If true, and if the \&quot;aggregators\&quot; analysisType is enabled, aggregators will be merged leniently.. [optional]  # noqa: E501
        """

        data_source = kwargs.get('data_source', nulltype.Null)
        intervals = kwargs.get('intervals', nulltype.Null)
        granularity = kwargs.get('granularity', nulltype.Null)
        dimension = kwargs.get('dimension', nulltype.Null)
        threshold = kwargs.get('threshold', nulltype.Null)
        metric = kwargs.get('metric', nulltype.Null)
        dimensions = kwargs.get('dimensions', nulltype.Null)
        _check_type = kwargs.pop('_check_type', True)
        _spec_property_naming = kwargs.pop('_spec_property_naming', False)
        _path_to_item = kwargs.pop('_path_to_item', ())
        _configuration = kwargs.pop('_configuration', None)
        _visited_composed_classes = kwargs.pop('_visited_composed_classes', ())

        if args:
            raise ApiTypeError(
                "Invalid positional arguments=%s passed to %s. Remove those invalid positional arguments." % (
                    args,
                    self.__class__.__name__,
                ),
                path_to_item=_path_to_item,
                valid_classes=(self.__class__,),
            )

        self._data_store = {}
        self._check_type = _check_type
        self._spec_property_naming = _spec_property_naming
        self._path_to_item = _path_to_item
        self._configuration = _configuration
        self._visited_composed_classes = _visited_composed_classes + (self.__class__,)

        constant_args = {
            '_check_type': _check_type,
            '_path_to_item': _path_to_item,
            '_spec_property_naming': _spec_property_naming,
            '_configuration': _configuration,
            '_visited_composed_classes': self._visited_composed_classes,
        }
        required_args = {
            'query_type': query_type,
            'data_source': data_source,
            'intervals': intervals,
            'granularity': granularity,
            'dimension': dimension,
            'threshold': threshold,
            'metric': metric,
            'dimensions': dimensions,
        }
        # remove args whose value is Null because they are unset
        required_arg_names = list(required_args.keys())
        for required_arg_name in required_arg_names:
            if required_args[required_arg_name] is nulltype.Null:
                del required_args[required_arg_name]
        model_args = {}
        model_args.update(required_args)
        model_args.update(kwargs)
        composed_info = validate_get_composed_info(
            constant_args, model_args, self)
        self._composed_instances = composed_info[0]
        self._var_name_to_model_instances = composed_info[1]
        self._additional_properties_model_instances = composed_info[2]
        unused_args = composed_info[3]

        for var_name, var_value in required_args.items():
            setattr(self, var_name, var_value)
        for var_name, var_value in kwargs.items():
            if var_name in unused_args and \
                        self._configuration is not None and \
                        self._configuration.discard_unknown_keys and \
                        not self._additional_properties_model_instances:
                # discard variable.
                continue
            setattr(self, var_name, var_value)

    @cached_property
    def _composed_schemas():
        # we need this here to make our import statements work
        # we must store _composed_schemas in here so the code is only run
        # when we invoke this method. If we kept this at the class
        # level we would get an error beause the class level
        # code would be run when this module is imported, and these composed
        # classes don't exist yet because their module has not finished
        # loading
        lazy_import()
        return {
          'anyOf': [
          ],
          'allOf': [
          ],
          'oneOf': [
              TelemetryDruidDataSourceMetadataRequest,
              TelemetryDruidGroupByRequest,
              TelemetryDruidScanRequest,
              TelemetryDruidSegmentMetadataRequest,
              TelemetryDruidTimeBoundaryRequest,
              TelemetryDruidTimeSeriesRequest,
              TelemetryDruidTopNRequest,
          ],
        }
